{
  "distributed": {
    "local_rank": 0,
    "rank": 0,
    "device": 0,
    "world_size": 0,
    "random_seed": 42
  },
  "optimizer": {
    "lr": 0.0002,
    "weight_decay": 0.01,
    "correct_bias": true,
    "adam_epsilon": 1e-6,
    "no_decay_bias": false,
    "adam_beta1": 0.9,
    "adam_beta2": 0.999
  },
  "scheduler": {
    "type": "linear",
    "max_step": null,
    "max_epoch": 5,
    "warmup_step": 500,
    "i_steps": "0",
    "i_lrs": "0.00025"
  },
  "data": {
    "train_data_c0": "./data/e2e/train0.jsonl",
    "train_data_c1": "./data/e2e/train1.jsonl",
    "train_data_c2": "./data/e2e/train2.jsonl",
    "valid_data": "./data/e2e/valid.jsonl",
    "train_batch_size": 4,
    "valid_batch_size": 4,
    "seq_len": 512
  },
  "training": {
    "grad_acc": 1,
    "clip": 0.0,
    "log_interval": 100,
    "eval_interval": 300,
    "save_interval": 200,
    "work_dir": "./trained_models/train-splitlora-gpt2sm-rank8"
  },
  "model": {
    "model_card": "gpt2.sm",
    "init_checkpoint": "./pretrained_checkpoints/gpt2-pytorch_model.bin",
    "obj": "clm",
    "label_smooth": 0.1,
    "split_point": 3
  },
  "lora": {
    "lora_dim": 8,
    "lora_alpha": 32,
    "lora_dropout": 0.1
  },
  "rolling": {
    "roll_interval": -1,
    "roll_lr": 0.00001,
    "roll_step": 100,
    "eval_epoch": 1
  },
  "wandb": {
    "logging": false,
    "run_name": "train-splitlora-gpt2.sm-rank8"
  }
}